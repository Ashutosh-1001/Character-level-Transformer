# Character-level-Transformer
This project implements a Transformer-based character-level language model in PyTorch for generating text. Unlike traditional word-level models, this approach learns patterns at the character level, making it suitable for creative text generation, low-resource languages, and synthetic data augmentation.


 Features
✅ Transformer from scratch – Implements self-attention, multi-head attention, and feed-forward layers.

✅ Character-level tokenization – Processes text without predefined word boundaries.

✅ Efficient training – Uses AdamW optimizer and dropout for better generalization.

✅ End-to-end pipeline – Includes training, inference, and dataset preprocessing scripts.



